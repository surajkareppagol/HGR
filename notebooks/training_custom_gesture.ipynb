{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Gesture Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 23:10:03.096340: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-25 23:10:03.096409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-25 23:10:03.098844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-25 23:10:03.109693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-25 23:10:04.993892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/Suraj/Documents/GitHub/HGR/venv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/Suraj/Documents/GitHub/HGR/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from mediapipe_model_maker import gesture_recognizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Dataset\n",
    "\n",
    "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (label_names) must be none. The none label represents any gesture that isn't classified as one of the other gestures.\n",
    "\n",
    "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/Rock-Paper-Scissors-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Rock-Paper-Scissors-Data\n",
      "['none', 'rock', 'paper', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "print(dataset_path)\n",
    "labels = []\n",
    "for i in os.listdir(dataset_path):\n",
    "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
    "    labels.append(i)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88582/544766458.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "\n",
    "for label in labels:\n",
    "  label_dir = os.path.join(dataset_path, label)\n",
    "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
    "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
    "  for i in range(NUM_EXAMPLES):\n",
    "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
    "    axs[i].get_xaxis().set_visible(False)\n",
    "    axs[i].get_yaxis().set_visible(False)\n",
    "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Load the dataset located at dataset_path by using the Dataset.from_folder method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
    "\n",
    "The HandDataPreprocessingParams class contains two configurable options for the data loading process:\n",
    "\n",
    "`shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
    "`min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
    "`Split the dataset`: 80% for training, 10% for validation, and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=dataset_path,\n",
    "    hparams=gesture_recognizer.HandDataPreprocessingParams(min_detection_confidence=0.7)\n",
    ")\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the Hyperparameters section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hand_embedding (InputLayer  [(None, 128)]             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 4)                 516       \n",
      " out (Dense)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1028 (4.02 KB)\n",
      "Trainable params: 772 (3.02 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Training the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Training the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "189/189 [==============================] - 3s 9ms/step - loss: 0.7857 - categorical_accuracy: 0.4286 - val_loss: 0.2779 - val_categorical_accuracy: 0.7872 - lr: 0.0030\n",
      "Epoch 2/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.5652 - categorical_accuracy: 0.5661 - val_loss: 0.2663 - val_categorical_accuracy: 0.8511 - lr: 0.0030\n",
      "Epoch 3/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.5005 - categorical_accuracy: 0.6323 - val_loss: 0.2728 - val_categorical_accuracy: 0.8511 - lr: 0.0029\n",
      "Epoch 4/40\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4766 - categorical_accuracy: 0.6481 - val_loss: 0.2541 - val_categorical_accuracy: 0.8298 - lr: 0.0029\n",
      "Epoch 5/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.4547 - categorical_accuracy: 0.6640 - val_loss: 0.2700 - val_categorical_accuracy: 0.8298 - lr: 0.0029\n",
      "Epoch 6/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.4377 - categorical_accuracy: 0.6455 - val_loss: 0.2799 - val_categorical_accuracy: 0.8298 - lr: 0.0029\n",
      "Epoch 7/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.4306 - categorical_accuracy: 0.6614 - val_loss: 0.2776 - val_categorical_accuracy: 0.8085 - lr: 0.0028\n",
      "Epoch 8/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.4144 - categorical_accuracy: 0.6984 - val_loss: 0.2634 - val_categorical_accuracy: 0.8085 - lr: 0.0028\n",
      "Epoch 9/40\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4144 - categorical_accuracy: 0.6693 - val_loss: 0.2565 - val_categorical_accuracy: 0.8085 - lr: 0.0028\n",
      "Epoch 10/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3922 - categorical_accuracy: 0.6931 - val_loss: 0.2634 - val_categorical_accuracy: 0.8085 - lr: 0.0027\n",
      "Epoch 11/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3921 - categorical_accuracy: 0.6905 - val_loss: 0.2794 - val_categorical_accuracy: 0.8085 - lr: 0.0027\n",
      "Epoch 12/40\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3729 - categorical_accuracy: 0.7169 - val_loss: 0.2983 - val_categorical_accuracy: 0.8085 - lr: 0.0027\n",
      "Epoch 13/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3701 - categorical_accuracy: 0.7143 - val_loss: 0.2745 - val_categorical_accuracy: 0.8085 - lr: 0.0027\n",
      "Epoch 14/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3675 - categorical_accuracy: 0.7143 - val_loss: 0.2873 - val_categorical_accuracy: 0.7872 - lr: 0.0026\n",
      "Epoch 15/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3680 - categorical_accuracy: 0.7169 - val_loss: 0.3025 - val_categorical_accuracy: 0.8085 - lr: 0.0026\n",
      "Epoch 16/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3715 - categorical_accuracy: 0.7063 - val_loss: 0.3009 - val_categorical_accuracy: 0.8298 - lr: 0.0026\n",
      "Epoch 17/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3599 - categorical_accuracy: 0.7249 - val_loss: 0.2957 - val_categorical_accuracy: 0.7872 - lr: 0.0026\n",
      "Epoch 18/40\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3462 - categorical_accuracy: 0.7328 - val_loss: 0.3020 - val_categorical_accuracy: 0.7872 - lr: 0.0025\n",
      "Epoch 19/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3717 - categorical_accuracy: 0.7037 - val_loss: 0.3185 - val_categorical_accuracy: 0.8085 - lr: 0.0025\n",
      "Epoch 20/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3530 - categorical_accuracy: 0.7249 - val_loss: 0.3297 - val_categorical_accuracy: 0.8085 - lr: 0.0025\n",
      "Epoch 21/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3622 - categorical_accuracy: 0.7196 - val_loss: 0.3179 - val_categorical_accuracy: 0.7872 - lr: 0.0025\n",
      "Epoch 22/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3506 - categorical_accuracy: 0.7593 - val_loss: 0.2940 - val_categorical_accuracy: 0.7872 - lr: 0.0024\n",
      "Epoch 23/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3451 - categorical_accuracy: 0.7407 - val_loss: 0.3272 - val_categorical_accuracy: 0.7872 - lr: 0.0024\n",
      "Epoch 24/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3407 - categorical_accuracy: 0.7302 - val_loss: 0.3423 - val_categorical_accuracy: 0.7872 - lr: 0.0024\n",
      "Epoch 25/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3513 - categorical_accuracy: 0.7249 - val_loss: 0.3183 - val_categorical_accuracy: 0.7872 - lr: 0.0024\n",
      "Epoch 26/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3284 - categorical_accuracy: 0.7302 - val_loss: 0.3468 - val_categorical_accuracy: 0.7872 - lr: 0.0023\n",
      "Epoch 27/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3489 - categorical_accuracy: 0.7275 - val_loss: 0.3629 - val_categorical_accuracy: 0.7660 - lr: 0.0023\n",
      "Epoch 28/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3467 - categorical_accuracy: 0.7196 - val_loss: 0.3328 - val_categorical_accuracy: 0.7872 - lr: 0.0023\n",
      "Epoch 29/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3325 - categorical_accuracy: 0.7672 - val_loss: 0.3412 - val_categorical_accuracy: 0.8085 - lr: 0.0023\n",
      "Epoch 30/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3306 - categorical_accuracy: 0.7275 - val_loss: 0.3288 - val_categorical_accuracy: 0.8085 - lr: 0.0022\n",
      "Epoch 31/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3297 - categorical_accuracy: 0.7434 - val_loss: 0.3356 - val_categorical_accuracy: 0.7660 - lr: 0.0022\n",
      "Epoch 32/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3264 - categorical_accuracy: 0.7751 - val_loss: 0.3385 - val_categorical_accuracy: 0.7872 - lr: 0.0022\n",
      "Epoch 33/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3397 - categorical_accuracy: 0.7249 - val_loss: 0.3443 - val_categorical_accuracy: 0.7660 - lr: 0.0022\n",
      "Epoch 34/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3280 - categorical_accuracy: 0.7328 - val_loss: 0.3295 - val_categorical_accuracy: 0.8085 - lr: 0.0022\n",
      "Epoch 35/40\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3365 - categorical_accuracy: 0.7460 - val_loss: 0.3258 - val_categorical_accuracy: 0.7660 - lr: 0.0021\n",
      "Epoch 36/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3249 - categorical_accuracy: 0.7566 - val_loss: 0.3324 - val_categorical_accuracy: 0.7660 - lr: 0.0021\n",
      "Epoch 37/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3233 - categorical_accuracy: 0.7540 - val_loss: 0.3186 - val_categorical_accuracy: 0.7660 - lr: 0.0021\n",
      "Epoch 38/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3221 - categorical_accuracy: 0.7646 - val_loss: 0.3425 - val_categorical_accuracy: 0.7660 - lr: 0.0021\n",
      "Epoch 39/40\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3176 - categorical_accuracy: 0.7778 - val_loss: 0.3439 - val_categorical_accuracy: 0.7660 - lr: 0.0020\n",
      "Epoch 40/40\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3118 - categorical_accuracy: 0.7513 - val_loss: 0.3197 - val_categorical_accuracy: 0.7660 - lr: 0.0020\n"
     ]
    }
   ],
   "source": [
    "hparams = gesture_recognizer.HParams(learning_rate=0.003, epochs=40, export_dir=\"../tasks\")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model performance\n",
    "\n",
    "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 4ms/step - loss: 0.2055 - categorical_accuracy: 0.8750\n",
      "Test loss:0.20546700060367584, Test accuracy:0.875\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data, batch_size=1)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Tensorflow Lite Model\n",
    "\n",
    "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpxt28goh3/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxt28goh3/saved_model/assets\n",
      "2024-03-25 23:20:14.852953: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-25 23:20:14.853034: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-25 23:20:14.853867: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpxt28goh3/saved_model\n",
      "2024-03-25 23:20:14.855780: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-25 23:20:14.855814: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpxt28goh3/saved_model\n",
      "2024-03-25 23:20:14.859106: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-03-25 23:20:14.860695: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-25 23:20:14.899826: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpxt28goh3/saved_model\n",
      "2024-03-25 23:20:14.913679: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 59814 microseconds.\n",
      "2024-03-25 23:20:14.949900: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 4, Total Ops 11, % non-converted = 36.36 %\n",
      " * 4 ARITH ops\n",
      "\n",
      "- arith.constant:    4 occurrences  (f32: 4)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "model.export_model(\"../tasks/gesture_recognizer.task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "You can further customize the model using the GestureRecognizerOptions class, which has two optional parameters for ModelOptions and HParams. Use the ModelOptions class to customize parameters related to the model itself, and the HParams class to customize other parameters related to training and saving the model.\n",
    "\n",
    "ModelOptions has one customizable parameter that affects accuracy:\n",
    "\n",
    "`dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
    "`layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
    "\n",
    "HParams has the following list of customizable parameters which affect model accuracy:\n",
    "\n",
    "`learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
    "`batch_size`: Batch size for training. Defaults to 2.\n",
    "`epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
    "`steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
    "`shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
    "`lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
    "`gamma`: Gamma parameter for focal loss. Defaults to 2\n",
    "\n",
    "Additional HParams parameter that does not affect model accuracy:\n",
    "\n",
    "`export_dir`: The location of the model checkpoint files and exported model files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
    "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
    "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
    "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_2.evaluate(test_data)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
